<section class="hero">
    <div class="container">
        <h1>資源｜Resources</h1>
    </div>
</section>

<div class="page-content resources-page glass-effect">
    <section class="container">
        <p>我們提供多種類型的資源，包括：</p>
        <p class="en">We provide various types of resources, including:</p>
        <div class="row justify-content-center">
            <div class="col-md-4">
                <div class="card h-100 glass-effect">
                    <div class="card-body d-flex flex-column">
                        <h5 class="card-title">文章｜Articles</h5>
                        <p class="card-text">由專家撰寫的關於 AI Safety 和應用的文章。</p>
                        <p class="card-text en">Expert-written articles on AI safety and applications.</p>
                        <a href="https://www.lesswrong.com/" class="btn btn-primary mt-auto" target="_blank">Less Wrong 論壇<br>Less Wrong Forum</a>
                    </div>
                </div>
            </div>
            <div class="col-md-4">
                <div class="card h-100 glass-effect">
                    <div class="card-body d-flex flex-column">
                        <h5 class="card-title">工具｜Tools</h5>
                        <p class="card-text">幫助開發者和研究人員進行 AI Safety 評估的實用工具。</p>
                        <p class="card-text en">Practical tools for developers and researchers to conduct AI safety assessments.</p>
                        <a href="https://www.neelnanda.io/mechanistic-interpretability/quickstart" class="btn btn-primary mt-auto" target="_blank">機制可解釋性快速入門<br>Mechanistic Interpretability Quickstart</a>
                    </div>
                </div>
            </div>
            <div class="col-md-4">
                <div class="card h-100 glass-effect">
                    <div class="card-body d-flex flex-column">
                        <h5 class="card-title">教程｜Tutorials</h5>
                        <p class="card-text">教學影片和指南，幫助用戶了解 AI Safety 的基本概念和實踐。</p>
                        <p class="card-text en">Educational videos and guides to help users understand AI safety concepts and practices.</p>
                        <a href="https://course.aisafetyfundamentals.com/alignment" class="btn btn-primary mt-auto" target="_blank">Blue Dot Impact 課程<br>Blue Dot Impact AI Alignment Course</a>
                    </div>
                </div>
            </div>
        </div>
    </section>
</div>

<div class="content-container glass-effect">
    <section class="container mt-5">
        <h2 class="section-title">我們的文章｜Our Articles</h2>
        <p>由 AI Safety Taiwan 團隊撰寫的原創文章和翻譯作品。</p>
        <p class="en">Original articles and translations written by the AI Safety Taiwan team.</p>

        <div class="row articles-grid mt-4">
            <div class="col-md-12">
                <div class="article-card glass-effect">
                    <div class="article-content">
                        <h3 class="article-title">Safe RLHF: Safe Reinforcement Learning from Human Feedback</h3>
                        <div class="article-meta">
                            <span class="article-tag">ICLR 2024 Spotlight</span>
                            <span class="article-date">2024</span>
                        </div>
                        <p class="article-excerpt">人類回饋強化學習（RLHF）是一種強大的對齊方法，但可能會產生意外或不安全的行為。本文介紹了 Safe RLHF，一種改進方法，有助於在保持高效能的同時增強 AI 系統的安全性。</p>
                        <p class="article-excerpt en">Reinforcement Learning from Human Feedback (RLHF) is a powerful alignment method but may produce unexpected or unsafe behaviors. This article introduces Safe RLHF, an improved approach that helps enhance the safety of AI systems while maintaining high performance.</p>
                        <a href="https://www.notion.so/Safe-RLHF-ICLR-2024-spotlight-1bfd49877c2a808c9fb0d546a9cae6c6" class="btn btn-primary mt-3" target="_blank">閱讀文章｜Read Article</a>
                    </div>
                </div>
            </div>
        </div>
    </section>
</div> 